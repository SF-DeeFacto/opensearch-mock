import pandas as pd
from opensearchpy import OpenSearch
import os
import datetime
import time
import threading

# OpenSearch ì„¤ì •
HOST = 'localhost'
PORT = 9200
USERNAME = 'admin'
PASSWORD = 'StrongPassword22!'

client = OpenSearch(
    hosts=[{'host': HOST, 'port': PORT}],
    http_auth=(USERNAME, PASSWORD),
    use_ssl=True,
    verify_certs=False,  # ê°œë°œ í™˜ê²½ì—ì„œë§Œ False í—ˆìš©
    ssl_assert_hostname=False,
    ssl_show_warn=False
)

# ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_index(index_name: str, index_body: dict):
    """ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œ í›„ ì¬ìƒì„±"""
    if client.indices.exists(index=index_name):
        print(f"Index '{index_name}' already exists. Deleting and recreating...")
        client.indices.delete(index=index_name)
    client.indices.create(index=index_name, body=index_body)
    print(f"Index '{index_name}' created.")

# ì„¼ì„œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
def stream_csv_data_to_opensearch(file_path: str):
    """CSV ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ì²˜ëŸ¼ OpenSearchë¡œ ì „ì†¡ (ì¼ë°˜ ì„¼ì„œ)"""
    if not os.path.exists(file_path):
        print(f"Error: CSV file not found at '{file_path}'")
        return

    print(f"Streaming from: {file_path}")
    df = pd.read_csv(file_path)
    print(f"Total {len(df)} records to stream.")

    for _, row in df.iterrows():
        current_timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat(timespec='milliseconds').replace('+00:00', 'Z')

        doc = {
            'id': str(row['id']),
            'sensor_id': str(row['sensor_id']),
            'zone_id': str(row['zone_id']),
            'timestamp': current_timestamp,
            'sensor_type': str(row['sensor_type']),
            'unit': str(row['unit']),
            'val': float(row['val'])
        }

        index_name = None
        if row['sensor_type'] == 'temperature':
            index_name = 'temp_sensor_data_stream'
        elif row['sensor_type'] == 'humidity':
            index_name = 'humi_sensor_data_stream'
        elif row['sensor_type'] == 'esd':
            index_name = 'esd_sensor_data_stream'
        elif row['sensor_type'] == 'windDir':
            index_name = 'winddir_sensor_data_stream'

        try:
            # ìˆ˜ì • í•„ìš”ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
            response = client.index(index=index_name, body=doc)
            print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] Inserted: {doc['sensor_id']} ({doc['val']})")
        except Exception as e:
            print(f"Error inserting document: {e}")

        time.sleep(1)

def stream_csv_data_to_opensearch_particle(file_path: str):
    """CSV ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ì²˜ëŸ¼ OpenSearchë¡œ ì „ì†¡ (ì…ì ì„¼ì„œ)"""
    if not os.path.exists(file_path):
        print(f"Error: CSV file not found at '{file_path}'")
        return

    print(f"Streaming from: {file_path}")
    df = pd.read_csv(file_path)
    print(f"Total {len(df)} records to stream.")

    for _, row in df.iterrows():
        current_timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat(timespec='milliseconds').replace('+00:00', 'Z')

        doc = {
            'id': str(row['id']),
            'sensor_id': str(row['sensor_id']),
            'zone_id': str(row['zone_id']),
            'timestamp': current_timestamp,
            'sensor_type': str(row['sensor_type']),
            'unit': str(row['unit']),
            'val_0_1': float(row['val_0_1']),
            'val_0_3': float(row['val_0_3']),
            'val_0_5': float(row['val_0_5'])
        }



        try:
            response = client.index(index='particle_sensor_data_stream', body=doc)
            print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] Inserted: {doc['sensor_id']} (0.1Î¼m: {doc['val_0_1']})")
        except Exception as e:
            print(f"Error inserting document: {e}")

        time.sleep(1)

# ë©”ì¸ ì‹¤í–‰ë¶€ (ì„¼ì„œ 5ê°œ ë°ì´í„°)
if __name__ == '__main__':
    # ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜
    index_body = {
        'settings': {'index': {'number_of_shards': 1}},
        'mappings': {
            'properties': {
                'id': {'type': 'keyword'},
                'sensor_id': {'type': 'keyword'},
                'zone_id': {'type': 'keyword'},
                'timestamp': {'type': 'date'},
                'sensor_type': {'type': 'keyword'},
                'unit': {'type': 'keyword'},
                'val': {'type': 'float'}
            }
        }
    }

    particle_index_body = {
        'settings': {'index': {'number_of_shards': 1}},
        'mappings': {
            'properties': {
                'id': {'type': 'keyword'},
                'sensor_id': {'type': 'keyword'},
                'zone_id': {'type': 'keyword'},
                'timestamp': {'type': 'date'},
                'sensor_type': {'type': 'keyword'},
                'unit': {'type': 'keyword'},
                'val_0_1': {'type': 'float'},
                'val_0_3': {'type': 'float'},
                'val_0_5': {'type': 'float'}
            }
        }
    }

    # ì¸ë±ìŠ¤ ìƒì„±
    create_index('temp_sensor_data_stream', index_body)
    create_index('humi_sensor_data_stream', index_body)
    create_index('esd_sensor_data_stream', index_body)
    create_index('winddir_sensor_data_stream', index_body)
    create_index('particle_sensor_data_stream', particle_index_body)

    # CSV íŒŒì¼ ëª©ë¡ ì •ì˜
    csv_files = [
        ('./data/sensor_data.csv', stream_csv_data_to_opensearch),
        ('./data/temp2_meta.csv', stream_csv_data_to_opensearch),
        ('./data/humi_meta.csv', stream_csv_data_to_opensearch),
        ('./data/humi2_meta.csv', stream_csv_data_to_opensearch),
        ('./data/lpm1_meta.csv', stream_csv_data_to_opensearch_particle),
        ('./data/lpm2_meta.csv', stream_csv_data_to_opensearch_particle),
        ('./data/wd1_meta.csv', stream_csv_data_to_opensearch),
        ('./data/wd2_meta.csv', stream_csv_data_to_opensearch),
        ('./data/esd1_meta.csv', stream_csv_data_to_opensearch),
        ('./data/esd2_meta.csv', stream_csv_data_to_opensearch),
    ]

    # ìŠ¤ë ˆë“œ ì‹¤í–‰
    threads = []
    for file_path, func in csv_files:
        thread = threading.Thread(target=func, args=(file_path,))
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()

    print("âœ… All CSV data streaming simulations complete.")
